{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Moving Data Between Host and Device\n",
    "\n",
    "#### Objective \n",
    "\n",
    "This tutorial shows how we can use the `map` clause with the `target data` directive to copy data to and from the GPU. \n",
    "\n",
    "#### Overview \n",
    "The CPU and GPU have separate memory spaces. When we want the GPU to access data allocated on the CPU (or _vice versa_), we need to copy the data from one memory to the other. By default OpenMP will copy all variables within lexical scope to and from the device. Notwithstanding, in certain cases we need to tell OpenMP which data we want copied. \n",
    "\n",
    "The code below performs a vector addition. The `target` directive has been used to offload the vector addition task to the GPU. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting vec_add.cpp\n"
     ]
    }
   ],
   "source": [
    "%%writefile vec_add.cpp\n",
    "#include<iostream>\n",
    "#include<omp.h>\n",
    "\n",
    "using namespace std;\n",
    "\n",
    "int main() {\n",
    "  unsigned N = 10000;\n",
    "  float A[N];\n",
    "  float B[N];\n",
    "  float C[N];\n",
    "\n",
    "  for (unsigned i = 0; i < N; i++) {\n",
    "    A[i] = i * 2.17;\n",
    "    B[i] = i * 3.14;\n",
    "    C[i] = 0;\n",
    "  }\n",
    "\n",
    "  #pragma omp target\n",
    "  {  \n",
    "    #pragma omp parallel for \n",
    "    for (unsigned i = 0; i < N; i++)\n",
    "      C[i] = A[i] + B[i];\n",
    "  }\n",
    "\n",
    "  cout << \"Computation Done!\" << endl;\n",
    "\n",
    "  // verify results                                                                                           \n",
    "  for (unsigned i = 1; i < 2; i++)\n",
    "    cout << \"C[1] = \" << C[i] << endl;\n",
    "\n",
    "  return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compile and run this code. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "!g++ -o vec_add -fopenmp vec_add.cpp  -fno-stack-protector -foffload=nvptx-none -fopenmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computation Done!\n",
      "C[1] = 5.31\n",
      "\n",
      " Performance counter stats for './vec_add':\n",
      "\n",
      "            224.66 msec task-clock                #    0.850 CPUs utilized          \n",
      "                58      context-switches          #    0.258 K/sec                  \n",
      "                 0      cpu-migrations            #    0.000 K/sec                  \n",
      "             3,472      page-faults               #    0.015 M/sec                  \n",
      "       629,850,790      cycles                    #    2.804 GHz                    \n",
      "       606,730,402      instructions              #    0.96  insn per cycle         \n",
      "       128,837,039      branches                  #  573.471 M/sec                  \n",
      "         2,773,679      branch-misses             #    2.15% of all branches        \n",
      "\n",
      "       0.264281673 seconds time elapsed\n",
      "\n",
      "       0.068586000 seconds user\n",
      "       0.157345000 seconds sys\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!perf stat ./vec_add"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The `target data` construct \n",
    "The example code uses static allocation for the array A, B, and C. This is very limiting and will rarely appear in practice. Let's modify the code to do dynamic allocation of A. We will save this version of the code as `vec_add_dynamic.cpp`  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting vec_add_dynamic.cpp\n"
     ]
    }
   ],
   "source": [
    "%%writefile vec_add_dynamic.cpp\n",
    "#include<iostream>\n",
    "#include<omp.h>\n",
    "\n",
    "using namespace std;\n",
    "\n",
    "int main() {\n",
    "  unsigned N = 10000;\n",
    "  float *A = (float *) malloc(sizeof(float) * N);\n",
    "  float B[N];\n",
    "  float C[N];\n",
    "\n",
    "  for (unsigned i = 0; i < N; i++) {\n",
    "    A[i] = i * 2.17;\n",
    "    B[i] = i * 3.14;\n",
    "    C[i] = 0;\n",
    "  }\n",
    "\n",
    "  #pragma omp target \n",
    "  {\n",
    "  \n",
    "    #pragma omp parallel for \n",
    "    for (unsigned i = 0; i < N; i++)\n",
    "      C[i] = A[i] + B[i];\n",
    "  }\n",
    "\n",
    "  cout << \"Computation Done!\" << endl;\n",
    "\n",
    "  // verify results                                                                                           \n",
    "  for (unsigned i = 1; i < 2; i++)\n",
    "    cout << \"C[1] = \" << C[i] << endl;\n",
    "\n",
    "  return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compile and run this version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "!g++ -o vec_add -fopenmp vec_add_dynamic.cpp  -fno-stack-protector -foffload=nvptx-none -fopenmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\n",
      "libgomp: cuCtxSynchronize error: an illegal memory access was encountered\r\n",
      "\r\n",
      "libgomp: cuMemFree_v2 error: an illegal memory access was encountered\r\n",
      "\r\n",
      "libgomp: device finalization failed\r\n"
     ]
    }
   ],
   "source": [
    "!./vec_add"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**_What happened?_**  The error message indicates that the GPU kernel is trying to access data that has not been allocated to GPU memory. Why did OpenMP not copy the `A` array? By default, OpenMP will copy, both to and from the device, all scalar variables and static arrays in scope. However, it will not copy dynamically allocated data. (The [OpenMP 4.5 specs](https://www.openmp.org/wp-content/uploads/openmp-examples-4.5.0.pdf) is a little unclear about this). \n",
    "\n",
    "So, we need to tell OpenMP to create copy 'A' to device. \n",
    "Essentially, this is telling OpenMP that we are doing dynamic memory allocation, so make sure there is enough space. And this is why we are getting the error. \n",
    "\n",
    "\n",
    "#### The `map` clause \n",
    "The `map` clause is used to explicitly map data to device memory. `map` takes a list of variables as its arugment and maps them to device memory. An optional qualifier can be specified to control _how_ data is mapped. More on this latrer. For this example, we want to map A to device memory. When mapping dyanamically allocated data, the number of elements that need to be mapped must also be specified. Bad things will happen otheriwse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting vec_add_dynamic.cpp\n"
     ]
    }
   ],
   "source": [
    "%%writefile vec_add_dynamic.cpp\n",
    "#include<iostream>\n",
    "#include<omp.h>\n",
    "\n",
    "using namespace std;\n",
    "\n",
    "int main() {\n",
    "  unsigned N = 10000;\n",
    "  float *A = (float *) malloc(sizeof(float) * N);\n",
    "  float B[N];\n",
    "  float C[N];\n",
    "\n",
    "  for (unsigned i = 0; i < N; i++) {\n",
    "    A[i] = i * 2.17;\n",
    "    B[i] = i * 3.14;\n",
    "  }\n",
    "\n",
    "  #pragma omp target map(A[0:N])\n",
    "  {\n",
    "    #pragma omp parallel for \n",
    "    for (unsigned i = 0; i < N; i++)\n",
    "      C[i] = A[i] + B[i];\n",
    "  }\n",
    "\n",
    "  cout << \"Computation Done!\" << endl;\n",
    "\n",
    "  // verify results                                                                                           \n",
    "  for (unsigned i = 1; i < 2; i++)\n",
    "    cout << \"C[1] = \" << C[i] << endl;\n",
    "\n",
    "  return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try out the corrected version. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "!g++ -o vec_add -fopenmp vec_add_dynamic.cpp  -fno-stack-protector -foffload=nvptx-none -fopenmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " GPU activities:   97.64%  1.6623ms         1  1.6623ms  1.6623ms  1.6623ms  main$_omp_fn$0\r\n"
     ]
    }
   ],
   "source": [
    "! /usr/local/cuda/bin/nvprof ./vec_add 2>&1 | grep main"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's allocate _B_ and _C_ in dynamic memory and add the appropiate `map` clauses. Note, we are still relying on OpenMP to implicitly map _N_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting vec_add_dynamic.cpp\n"
     ]
    }
   ],
   "source": [
    "%%writefile vec_add_dynamic.cpp\n",
    "#include<iostream>\n",
    "#include<omp.h>\n",
    "\n",
    "using namespace std;\n",
    "\n",
    "int main() {\n",
    "  unsigned N = 10000;\n",
    "  float *A = (float *) malloc(sizeof(float) * N);\n",
    "  float *B = (float *) malloc(sizeof(float) * N);\n",
    "  float *C = (float *) malloc(sizeof(float) * N);\n",
    "\n",
    "  for (unsigned i = 0; i < N; i++) {\n",
    "    A[i] = i * 2.17;\n",
    "    B[i] = i * 3.14;\n",
    "  }\n",
    "\n",
    "  #pragma omp target map(A[0:N],B[0:N],C[0:N])\n",
    "  {\n",
    "    #pragma omp parallel for \n",
    "    for (unsigned i = 0; i < N; i++)\n",
    "      C[i] = A[i] + B[i];\n",
    "  }\n",
    "\n",
    "  cout << \"Computation Done!\" << endl;\n",
    "\n",
    "  // verify results                                                                                           \n",
    "  for (unsigned i = 1; i < 2; i++)\n",
    "    cout << \"C[1] = \" << C[i] << endl;\n",
    "  \n",
    "  return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "!g++ -o vec_add -fopenmp vec_add_dynamic.cpp  -fno-stack-protector -foffload=nvptx-none -fopenmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " GPU activities:   98.70%  1.6634ms         1  1.6634ms  1.6634ms  1.6634ms  main$_omp_fn$0\r\n",
      "                    0.96%  16.097us         3  5.3650us     832ns  7.6800us  [CUDA memcpy HtoD]\r\n",
      "                    0.34%  5.7930us         1  5.7930us  5.7930us  5.7930us  [CUDA memcpy DtoH]\r\n",
      "                    0.02%  60.233us         3  20.077us  12.680us  25.180us  cuMemcpyHtoD\r\n",
      "                    0.02%  51.392us         1  51.392us  51.392us  51.392us  cuMemcpyDtoH\r\n"
     ]
    }
   ],
   "source": [
    "!/usr/local/cuda/bin/nvprof ./vec_add 2>&1 | grep \"main\\|HtoD\\|DtoH\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Controlling `map` behavior \n",
    "\n",
    "Be default OpenMP will copy all mapped data from the CPU to the GPU at the beginning of the offloaded and then copy everything back at the end of the taks. This may lead to many unnecessary copies. We can optimize this behavior with modifiers in the `map` clause. \n",
    "\n",
    "The `map` clause accepts a modifier that allows us to specify the direction of data movement. In the code below, the says that the _A_ and _B_ arrays should be copied _to_ device memory when executing the offloaded task while the _C_ array should be copied _from_ device memory to host memory. For the vector add computation, this makes sense. The initialized values in _A_ and _B_ are copied to the GPU. We do not need to copy them back since the GPU doesn't modify these arrays. On the other hand, we do not need to copy _C_ to GPU but we did not to copy it back to the CPU to access the values updated by the GPU. \n",
    "\n",
    "If there is data that is both read from and written to by the GPU, we can just use `tofrom` modifier. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting vec_add_dynamic.cpp\n"
     ]
    }
   ],
   "source": [
    "%%writefile vec_add_dynamic.cpp\n",
    "#include<iostream>\n",
    "#include<omp.h>\n",
    "\n",
    "using namespace std;\n",
    "\n",
    "int main() {\n",
    "  unsigned N = 10000;\n",
    "  float *A = (float *) malloc(sizeof(float) * N);\n",
    "  float *B = (float *) malloc(sizeof(float) * N);\n",
    "  float *C = (float *) malloc(sizeof(float) * N);\n",
    "\n",
    "  for (unsigned i = 0; i < N; i++) {\n",
    "    A[i] = i * 2.17;\n",
    "    B[i] = i * 3.14;\n",
    "  }\n",
    "\n",
    "  #pragma omp target map(to:A[0:N],B[0:N]) map(from:C[0:N])\n",
    "  {\n",
    "     #pragma omp parallel for \n",
    "    for (unsigned i = 0; i < N; i++)\n",
    "      C[i] = A[i] + B[i];\n",
    "  }\n",
    "\n",
    "  cout << \"Computation Done!\" << endl;\n",
    "\n",
    "  // verify results                                                                                           \n",
    "  for (unsigned i = 1; i < 2; i++)\n",
    "    cout << \"C[1] = \" << C[i] << endl;\n",
    "  \n",
    "  return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "!g++ -o vec_add -fopenmp vec_add_dynamic.cpp  -fno-stack-protector -foffload=nvptx-none -fopenmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " GPU activities:   98.70%  1.6626ms         1  1.6626ms  1.6626ms  1.6626ms  main$_omp_fn$0\r\n"
     ]
    }
   ],
   "source": [
    "!/usr/local/cuda/bin/nvprof ./vec_add 2>&1 | grep \"main\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Summary \n",
    "\n",
    "In this tutorial we saw how we can use the `map` clause to copy data to and from the GPU device when executing an offloaded task. The `map` clause is necessary whenever we are accessing dynamically allocated data. For static data structures and scalar variables, OpenMP will do the mapping implicitly. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
